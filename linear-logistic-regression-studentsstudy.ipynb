{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6051848,"sourceType":"datasetVersion","datasetId":3462333},{"sourceId":9985284,"sourceType":"datasetVersion","datasetId":6144877}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Linear Regression","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nimport scipy.stats as stats\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:15.593092Z","iopub.execute_input":"2025-04-09T14:01:15.593364Z","iopub.status.idle":"2025-04-09T14:01:18.169244Z","shell.execute_reply.started":"2025-04-09T14:01:15.593338Z","shell.execute_reply":"2025-04-09T14:01:18.168226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = '/kaggle/input/student-performance-multiple-linear-regression/Student_Performance.csv'\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:18.170979Z","iopub.execute_input":"2025-04-09T14:01:18.171492Z","iopub.status.idle":"2025-04-09T14:01:18.414308Z","shell.execute_reply.started":"2025-04-09T14:01:18.171456Z","shell.execute_reply":"2025-04-09T14:01:18.413222Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Exploration and Preprocessing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv(path)\ndf.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:18.415731Z","iopub.execute_input":"2025-04-09T14:01:18.415985Z","iopub.status.idle":"2025-04-09T14:01:18.47002Z","shell.execute_reply.started":"2025-04-09T14:01:18.415962Z","shell.execute_reply":"2025-04-09T14:01:18.468832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:18.471221Z","iopub.execute_input":"2025-04-09T14:01:18.471631Z","iopub.status.idle":"2025-04-09T14:01:18.477358Z","shell.execute_reply.started":"2025-04-09T14:01:18.471558Z","shell.execute_reply":"2025-04-09T14:01:18.476317Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**the describtion of the data to see if there are outliers or not or is there any weird tendances in the data**","metadata":{}},{"cell_type":"code","source":"df.describe()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:18.478384Z","iopub.execute_input":"2025-04-09T14:01:18.47879Z","iopub.status.idle":"2025-04-09T14:01:18.522316Z","shell.execute_reply.started":"2025-04-09T14:01:18.478739Z","shell.execute_reply":"2025-04-09T14:01:18.521337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:18.525194Z","iopub.execute_input":"2025-04-09T14:01:18.525486Z","iopub.status.idle":"2025-04-09T14:01:18.555753Z","shell.execute_reply.started":"2025-04-09T14:01:18.525459Z","shell.execute_reply":"2025-04-09T14:01:18.554636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#checking for nulls and duplicates\nprint(f\"Number of nulls: {df.isnull().sum().sum()}\")\n\nprint(f\"Number of duplicates: {df.duplicated().sum()}\")\n\ndf = df.drop_duplicates()\nprint(f\"Number of duplicates after dropping: {df.duplicated().sum()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:18.557657Z","iopub.execute_input":"2025-04-09T14:01:18.558018Z","iopub.status.idle":"2025-04-09T14:01:18.581376Z","shell.execute_reply.started":"2025-04-09T14:01:18.557982Z","shell.execute_reply":"2025-04-09T14:01:18.580352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, LabelEncoder\n\n# Encode categorical column as linear Regression needs the features to be numerical\nle = LabelEncoder()\ndf['Extracurricular Activities'] = le.fit_transform(df['Extracurricular Activities'])\n\n#label encoder gives and index to each unique category it the feature\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:18.582485Z","iopub.execute_input":"2025-04-09T14:01:18.582798Z","iopub.status.idle":"2025-04-09T14:01:18.589461Z","shell.execute_reply.started":"2025-04-09T14:01:18.58277Z","shell.execute_reply":"2025-04-09T14:01:18.588429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#correlation matrix to see the correlations and dependancies between features if there are high correlations between 2 features then they are redundant and we can remove one of them\ncorrelation_matrix = df.corr(numeric_only=True)\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\nplt.title('Correlation Matrix for Numerical Features')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:18.590315Z","iopub.execute_input":"2025-04-09T14:01:18.59068Z","iopub.status.idle":"2025-04-09T14:01:19.206636Z","shell.execute_reply.started":"2025-04-09T14:01:18.590644Z","shell.execute_reply":"2025-04-09T14:01:19.205448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#We should drop sleep hours and sample Questions papers and Extracurricular Activities as they are not well correlated to the performance index\n#df.drop(columns=['Extracurricular Activities','Sleep Hours','Sample Question Papers Practiced'], inplace=True) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:19.207786Z","iopub.execute_input":"2025-04-09T14:01:19.208204Z","iopub.status.idle":"2025-04-09T14:01:19.212428Z","shell.execute_reply.started":"2025-04-09T14:01:19.208132Z","shell.execute_reply":"2025-04-09T14:01:19.211086Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Split the data and Normalization","metadata":{}},{"cell_type":"code","source":"# Features and target\ny = df['Performance Index']\nX = df.drop('Performance Index', axis=1)\n\n# Standardize the features we do this so the model learns easier and no feature overpowers the other\n\nscaler = StandardScaler()\nX_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns) #now all features are on the same playing field the model won't favour one over the other\n\n# Split the data\nfrom sklearn.model_selection import train_test_split as tts\nX_train, X_test, y_train, y_test = tts(X_scaled, y, test_size=0.1, random_state=42)\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:19.2136Z","iopub.execute_input":"2025-04-09T14:01:19.213973Z","iopub.status.idle":"2025-04-09T14:01:19.268465Z","shell.execute_reply.started":"2025-04-09T14:01:19.213922Z","shell.execute_reply":"2025-04-09T14:01:19.26745Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Linear Regression Models","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Single feature\nfeature = 'Previous Scores'\nX_single_train = X_train[[feature]]\nX_single_test = X_test[[feature]]\n\nlr_single = LinearRegression() #here we call the model\nlr_single.fit(X_single_train, y_train)  #here we are fitting/training the model on our data\n\ny_pred_single = lr_single.predict(X_single_test)  #and here we predict on the unseen test set\n\n\n# Plot\nplt.scatter(X_single_test, y_test, color='blue', label='Actual')\nplt.plot(X_single_test, y_pred_single, color='red', label='Predicted')\nplt.title('Single Linear Regression')\nplt.xlabel(feature)\nplt.ylabel('Performance Index')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Metrics\nr2_single = r2_score(y_test, y_pred_single)\nmse_single = mean_squared_error(y_test, y_pred_single)\nprint(f\"Single Linear Regression - RÂ² Score: {r2_single:.4f}\")\nprint(f\"Single Linear Regression - Mean Squared Error: {mse_single:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:19.269494Z","iopub.execute_input":"2025-04-09T14:01:19.269816Z","iopub.status.idle":"2025-04-09T14:01:19.566341Z","shell.execute_reply.started":"2025-04-09T14:01:19.26979Z","shell.execute_reply":"2025-04-09T14:01:19.565135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# All features Multi Linear regression\nlr_multi = LinearRegression()\nlr_multi.fit(X_train, y_train)\n\ny_pred_multi = lr_multi.predict(X_test)\n\n# Plot\nplt.scatter(y_test, y_pred_multi, alpha=0.5, color='green')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linewidth=2)\nplt.title('Multiple Linear Regression')\nplt.xlabel('Actual Performance Index')\nplt.ylabel('Predicted Performance Index')\nplt.grid(True)\nplt.show()\n\n# Metrics\nr2_multi = r2_score(y_test, y_pred_multi)\nmse_multi = mean_squared_error(y_test, y_pred_multi)\nprint(f\"Multiple Linear Regression - RÂ² Score: {r2_multi:.4f}\")\nprint(f\"Multiple Linear Regression - Mean Squared Error: {mse_multi:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:19.567346Z","iopub.execute_input":"2025-04-09T14:01:19.567702Z","iopub.status.idle":"2025-04-09T14:01:19.779047Z","shell.execute_reply.started":"2025-04-09T14:01:19.567673Z","shell.execute_reply":"2025-04-09T14:01:19.77799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfeatures = X_train.columns\n\ncoefficients = pd.DataFrame({\n    'Feature': features,\n    'Coefficient': lr_multi.coef_\n}).sort_values(by='Coefficient', ascending=False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Coefficient', y='Feature', data=coefficients, palette='viridis')\nplt.title(\"Feature Importance in Multiple Linear Regression\")\nplt.xlabel(\"Coefficient Value\")\nplt.ylabel(\"Feature\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n#here we visualize the feature importance like how we have seen from the covariance matrix ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:19.780243Z","iopub.execute_input":"2025-04-09T14:01:19.780639Z","iopub.status.idle":"2025-04-09T14:01:20.021507Z","shell.execute_reply.started":"2025-04-09T14:01:19.7806Z","shell.execute_reply":"2025-04-09T14:01:20.020461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Store results for each degree\nresults = {}\ndegrees = [2, 3, 10]\n\n# Loop through degrees\nfor degree in degrees:\n    # Polynomial transformation\n    poly = PolynomialFeatures(degree=degree)\n    X_poly_train = poly.fit_transform(X_train)\n    X_poly_test = poly.transform(X_test)\n\n    # Train model\n    model = LinearRegression()\n    model.fit(X_poly_train, y_train)\n\n    # Predict\n    y_pred = model.predict(X_poly_test)\n\n    # Evaluate\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n\n    # Save results\n    results[degree] = {\n        'y_pred': y_pred,\n        'mse': mse,\n        'r2': r2\n    }\n\n# Plot all 3 predictions\nfig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\nfor idx, degree in enumerate(degrees):\n    ax = axes[idx]\n    y_pred = results[degree]['y_pred']\n\n    ax.scatter(y_test, y_pred, alpha=0.6, label=f'Degree {degree}', color=f'C{idx}')\n    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n    ax.set_title(f'Polynomial Degree {degree}')\n    ax.set_xlabel('Actual Performance Index')\n    if idx == 0:\n        ax.set_ylabel('Predicted Performance Index')\n    ax.legend()\n    ax.grid(True)\n\nplt.suptitle('Polynomial Regression with All Features', fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\n# Print scores\nfor degree in degrees:\n    print(f\"Degree {degree}:\")\n    print(f\"  RÂ² Score: {results[degree]['r2']:.4f}\")\n    print(f\"  Mean Squared Error: {results[degree]['mse']:.4f}\")\n    print()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:20.022487Z","iopub.execute_input":"2025-04-09T14:01:20.022806Z","iopub.status.idle":"2025-04-09T14:01:30.864665Z","shell.execute_reply.started":"2025-04-09T14:01:20.022776Z","shell.execute_reply":"2025-04-09T14:01:30.863631Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* At first, increasing the model complexity (e.g., increasing the degree of polynomial features, using a more complex model like a higher-order polynomial regression or a more powerful machine learning algorithm) can improve the performance because the model is better able to capture the underlying patterns in the data\n* However, as the model becomes more complex, it may start to capture not just the true underlying patterns but also the noise in the training data. This means that the model fits the specific characteristics of the training data too closely and doesn't generalize well to unseen data. This leads to overfitting.\n\n* Overfitting occurs when a model performs well on training data but poorly on test data (or new, unseen data). This is because the model has learned to model the training data too well, including noise and outliers, which don't generalize to new data.","metadata":{}},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"## imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:30.865819Z","iopub.execute_input":"2025-04-09T14:01:30.866208Z","iopub.status.idle":"2025-04-09T14:01:30.871098Z","shell.execute_reply.started":"2025-04-09T14:01:30.866169Z","shell.execute_reply":"2025-04-09T14:01:30.870123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/student-depression-dataset/Student Depression Dataset.csv')\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:30.872077Z","iopub.execute_input":"2025-04-09T14:01:30.872428Z","iopub.status.idle":"2025-04-09T14:01:31.020074Z","shell.execute_reply.started":"2025-04-09T14:01:30.872391Z","shell.execute_reply":"2025-04-09T14:01:31.019103Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exploring and preprocessing of the data","metadata":{}},{"cell_type":"code","source":"print(data.info())\nprint(data.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:31.024245Z","iopub.execute_input":"2025-04-09T14:01:31.024579Z","iopub.status.idle":"2025-04-09T14:01:31.07696Z","shell.execute_reply.started":"2025-04-09T14:01:31.024548Z","shell.execute_reply":"2025-04-09T14:01:31.076005Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Count Duplicates :\" , data.duplicated().sum())\nprint(\"Count null values:\")\nprint(data.isnull().sum())\ndata.dropna(inplace=True)\nprint(data.isnull().sum())\n\nsns.heatmap(data.isnull())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:31.078542Z","iopub.execute_input":"2025-04-09T14:01:31.078819Z","iopub.status.idle":"2025-04-09T14:01:32.301643Z","shell.execute_reply.started":"2025-04-09T14:01:31.078791Z","shell.execute_reply":"2025-04-09T14:01:32.300714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.drop([\"id\"], axis=1, inplace=True)  # Remove irrelevant column\ndata.dropna(inplace=True)\ndata.drop_duplicates(inplace=True)\ndata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:32.302711Z","iopub.execute_input":"2025-04-09T14:01:32.302964Z","iopub.status.idle":"2025-04-09T14:01:32.365483Z","shell.execute_reply.started":"2025-04-09T14:01:32.302941Z","shell.execute_reply":"2025-04-09T14:01:32.364509Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['Depression'].value_counts()   #okay none balanced data shoud either downsample or upsample\n#garabt a3ml downsample gab 83 acc\n#upsampling gab 85 acc logistic bs howa fara2 fel knn gabet 77acc fa a win is a win\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:32.36637Z","iopub.execute_input":"2025-04-09T14:01:32.366654Z","iopub.status.idle":"2025-04-09T14:01:32.375674Z","shell.execute_reply.started":"2025-04-09T14:01:32.36663Z","shell.execute_reply":"2025-04-09T14:01:32.374701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['Depression'] = data['Depression'].astype('category',copy=False)\ncolors = ['purple', 'red']\nplot = data['Depression'].value_counts().plot(kind='bar', title=\"Class distributions \\n(0: Not Depressed | 1: Depressed)\", color=colors)\nfig = plot.get_figure()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:32.376643Z","iopub.execute_input":"2025-04-09T14:01:32.376892Z","iopub.status.idle":"2025-04-09T14:01:32.627891Z","shell.execute_reply.started":"2025-04-09T14:01:32.37687Z","shell.execute_reply":"2025-04-09T14:01:32.626886Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.utils import resample\n\n# Separate classes\nmajority = data[data['Depression'] == 1]\nminority = data[data['Depression'] == 0]\n\n# Upsample minority class\nminority_upsampled = resample(minority,\n                              replace=True,\n                              n_samples=len(majority),\n                              random_state=42)\n\n# Combine with majority class\ndata_balanced = pd.concat([majority, minority_upsampled])\n\n# Shuffle the result\ndata = data_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Check balance\nprint(data['Depression'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:32.628883Z","iopub.execute_input":"2025-04-09T14:01:32.629236Z","iopub.status.idle":"2025-04-09T14:01:32.658076Z","shell.execute_reply.started":"2025-04-09T14:01:32.629202Z","shell.execute_reply":"2025-04-09T14:01:32.657114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['Depression'] = data['Depression'].astype('category',copy=False)\ncolors = ['purple', 'red']\nplot = data['Depression'].value_counts().plot(kind='bar', title=\"Class distributions \\n(0: Not Depressed | 1: Depressed)\", color=colors)\nfig = plot.get_figure()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:32.658981Z","iopub.execute_input":"2025-04-09T14:01:32.659372Z","iopub.status.idle":"2025-04-09T14:01:32.860127Z","shell.execute_reply.started":"2025-04-09T14:01:32.659337Z","shell.execute_reply":"2025-04-09T14:01:32.859159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the correlation matrix for numerical features\nnumeric_data = data.select_dtypes(include=np.number)\ncorrelation_matrix = numeric_data.corr()\n\n# Display the correlation matrix using a heatmap\nplt.figure(figsize=(12, 10))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Matrix of Numerical Features')\nplt.show()\n\n#7ane3ml remove le either work pressure aw job satisfaction highly correlated redundant feature\ndata.drop(['Work Pressure'], axis=1, inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:32.860976Z","iopub.execute_input":"2025-04-09T14:01:32.861314Z","iopub.status.idle":"2025-04-09T14:01:33.298201Z","shell.execute_reply.started":"2025-04-09T14:01:32.861288Z","shell.execute_reply":"2025-04-09T14:01:33.297316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 10))\nnumerical_features = ['Academic Pressure', 'Age', 'CGPA', 'Financial Stress', 'Work/Study Hours','Study Satisfaction']\nfor i, feature in enumerate(numerical_features, 1):\n    plt.subplot(3, 3, i)\n    sns.boxplot(x='Depression', y=feature, data=data, palette=colors)\n    plt.title(f'{feature} Distribution by Depression Status')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:33.299198Z","iopub.execute_input":"2025-04-09T14:01:33.299545Z","iopub.status.idle":"2025-04-09T14:01:34.428948Z","shell.execute_reply.started":"2025-04-09T14:01:33.299494Z","shell.execute_reply":"2025-04-09T14:01:34.427805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numerical_features = ['Academic Pressure', 'Age', 'CGPA', 'Financial Stress', 'Work/Study Hours','Study Satisfaction']\n\nfor feature in numerical_features:\n    Q1 = data[feature].quantile(0.25)\n    Q3 = data[feature].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n\n    outliers = data[(data[feature] < lower_bound) | (data[feature] > upper_bound)]\n    print(f\"{feature}: {len(outliers)} outliers\")\n#mashy fy outliers bs msh keteer w kman they hold info fa msh 7anshelhom\n#Extreme values (e.g. very high academic pressure or very low CGPA) could be early indicators or strong correlates of depression.\n#Removing these points might bias your model toward average cases and reduce its ability to detect serious situations.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:34.429933Z","iopub.execute_input":"2025-04-09T14:01:34.430335Z","iopub.status.idle":"2025-04-09T14:01:34.457878Z","shell.execute_reply.started":"2025-04-09T14:01:34.430298Z","shell.execute_reply":"2025-04-09T14:01:34.456924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\nprint(data.info())\n#i want to drop  'Other' rows with other as they provide no information\ndata.drop(data[data['City'] == 'Other'].index, inplace=True)\ndata.drop(data[data['Sleep Duration'] == 'Other'].index, inplace=True)\ndata.drop(data[data['Dietary Habits'] == 'Other'].index, inplace=True)\ndata.drop(data[data['Degree'] == 'Other'].index, inplace=True)\ndata[\"City\"] = labelencoder.fit_transform(data[\"City\"])\ndata[\"Depression\"] = labelencoder.fit_transform(data[\"Depression\"])\n\ndata[\"Gender\"] = labelencoder.fit_transform(data[\"Gender\"])\ndata[\"Sleep Duration\"] = labelencoder.fit_transform(data[\"Sleep Duration\"])\ndata[\"Dietary Habits\"] = labelencoder.fit_transform(data[\"Dietary Habits\"])\ndata[\"Have you ever had suicidal thoughts ?\"] = labelencoder.fit_transform(data[\"Have you ever had suicidal thoughts ?\"])\ndata[\"Family History of Mental Illness\"] = labelencoder.fit_transform(data[\"Family History of Mental Illness\"])\ndata[\"Degree\"] = labelencoder.fit_transform(data[\"Degree\"])\ndata.head()\n\ndata.drop(['Profession'], axis=1, inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:34.458878Z","iopub.execute_input":"2025-04-09T14:01:34.459276Z","iopub.status.idle":"2025-04-09T14:01:34.558235Z","shell.execute_reply.started":"2025-04-09T14:01:34.45924Z","shell.execute_reply":"2025-04-09T14:01:34.557077Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Splitting and normalization","metadata":{}},{"cell_type":"code","source":"# Prepare the model\ny = data[\"Depression\"] # our target variable\nX = data.drop([\"Depression\"], axis=1) # our predictors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:34.559261Z","iopub.execute_input":"2025-04-09T14:01:34.559562Z","iopub.status.idle":"2025-04-09T14:01:34.566559Z","shell.execute_reply.started":"2025-04-09T14:01:34.55951Z","shell.execute_reply":"2025-04-09T14:01:34.565476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a scaler object\nscaler = StandardScaler()\n\n# Fit the scaler to the data and transform the data\nX_scaled = data[['Academic Pressure','Age','CGPA','Financial Stress','Work/Study Hours','Job Satisfaction','Study Satisfaction']]\nX_scaled = scaler.fit_transform(data[['Academic Pressure','Age','CGPA','Financial Stress','Work/Study Hours','Job Satisfaction','Study Satisfaction']])\n# X_scaled is now a numpy array with normalized data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:34.567448Z","iopub.execute_input":"2025-04-09T14:01:34.56778Z","iopub.status.idle":"2025-04-09T14:01:34.584881Z","shell.execute_reply.started":"2025-04-09T14:01:34.567751Z","shell.execute_reply":"2025-04-09T14:01:34.583853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:34.585857Z","iopub.execute_input":"2025-04-09T14:01:34.586185Z","iopub.status.idle":"2025-04-09T14:01:34.597565Z","shell.execute_reply.started":"2025-04-09T14:01:34.586149Z","shell.execute_reply":"2025-04-09T14:01:34.596492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:34.598625Z","iopub.execute_input":"2025-04-09T14:01:34.598927Z","iopub.status.idle":"2025-04-09T14:01:34.611126Z","shell.execute_reply.started":"2025-04-09T14:01:34.598902Z","shell.execute_reply":"2025-04-09T14:01:34.610015Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Classification models and Comparing between them","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\n\n\n\nmodels = {\n    'Logistic Regression': LogisticRegression(),\n    'Naive Bayes': GaussianNB(),\n    'K-Nearest Neighbors': KNeighborsClassifier()}\n\n\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(f\"--- {name} ---\")\n    print(classification_report(y_test, y_pred))\n    print(\"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:34.612088Z","iopub.execute_input":"2025-04-09T14:01:34.612351Z","iopub.status.idle":"2025-04-09T14:01:35.41053Z","shell.execute_reply.started":"2025-04-09T14:01:34.612328Z","shell.execute_reply":"2025-04-09T14:01:35.409604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#confusion matrices for each model\nfor name, model in models.items():\n    y_pred = model.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred)\n    disp = ConfusionMatrixDisplay(cm, display_labels=model.classes_)\n    disp.plot()\n    plt.title(f\"Confusion Matrix - {name}\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:01:35.411461Z","iopub.execute_input":"2025-04-09T14:01:35.411761Z","iopub.status.idle":"2025-04-09T14:01:36.286224Z","shell.execute_reply.started":"2025-04-09T14:01:35.411735Z","shell.execute_reply":"2025-04-09T14:01:36.284999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}